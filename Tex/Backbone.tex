\chapter{基础网络的研究与选择}

\section{主流基础网络特性分析}
目前情况下，包括图像检索在内的许多计算机视觉任务，在大部分情况下都会使用常用的基础分类网络作为其骨干网络充当特征提取器，这些主流的分类网络的性能已经在ImageNet得到证明。最经典的卷积神经网络是Yann LeCun在1998年设计并提出LeNet，这个用于识别手写字符的网络规模较小，但是包含了现在卷积神经网络的最基本组件：卷积层，池化层，全连接层。Alex Krizhevsky于2012年提出的AlexNet是卷积神经网络的一大步：AlexNet使用ReLU取代Sigmoid作为激活函数，成功解决了Sigmoid在网络较深时的梯度弥散问题；训练时采用了Dropout策略，随机忽略一部分神经元，可以有效避免模型的过拟合；引入了最大池化层而不是像之前只使用平均池化层，这有效的提升了特征的丰富性；使用CUDA加速神经网络训练，有效利用了GPU的计算能力。AlexNet被提出之后，深度学习飞速发展，越来越多性能优异的基础网络随之被提出，下面简要分析几个主流的基础网络：

1.VGG：相比AlexNet，VGG的一个改进是采用连续的几个3x3的卷积核代替AlexNet中的11x11或者5x5的较大卷积核。对于给定的感受野，采用堆积的小卷积核是优于采用大的卷积核，因为多层非线性层可以增加网络深度来保证学习更复杂的模式，此外小的卷积核也意味着更少的参数量，更低的计算复杂度。总结性的来说，VGG在控制计算量增长的同时将网络架构做的更深更宽，分类效果显著提升。
\input{Img/vgg}

2.Inception：又被称为GoogLeNet，实际上Inception指的是GoogLeNet的核心结构。一般来说提高网络表达能力最直接的方法就是增加网络的深度和宽度，但是直接这么做会带来一些问题：
\begin{itemize}
\item [(1)] 相对更深更宽的网络不可避免的会增加参数量，而过多的参数量容易导致过拟合。
\item [(2)] 随着网络的深度的增加，反向传播时会出现梯度消失的问题，导致网络很难优化。
\item [(3)] 计算量增加，会消耗更多的计算资源。
\end{itemize}

Inception结构针对限制神经网络性能的主要问题对传统的卷积策略不断改进，Inception v1设计出了多路并行的卷积模块，不同分支的卷积核大小不同，分别有1x1,3x3,5x5三种尺度，这么做的好处是可以以不同大小的感受野去学习得到不同的特征，卷积操作之后，不同分支的特征通过拼接的方式做融合。
\input{Img/inception-v1}
Inception v2\cite{ioffe2015batch}提出了有重大意义的BN(Batch Normalization)。训练深度神经网络时，作者抛出一个叫做“Internal Covariate Shift”的问题，这个问题指在训练过程中，第n层的输入就是第n-1层的输出，在训练过程中，每训练一轮参数就会发生变化，对于一个网络相同的输入，但n-1层的输出却不一样，这就导致第n层的输入也不一样，BN的提出就是为了解决这个问题。在传统机器学习中，对图像提取特征之前，都会对图像做白化操作，即对输入数据变换成0均值、单位方差的正态分布，卷积神经网络的输入就是图像，白化操作可以加快收敛，对于深度网络，每个隐层的输出都是下一个隐层的输入，即每个隐层的输入都可以做白化操作，BN就是在训练中的每个mini-batch上做了白化，可以有效防止梯度消失并加速网络训练。

3.ResNet：由微软研究院的Kaiming He等提出的ResNet，通过使用ResNet Unit成功训练出了152层的神经网络，其效果非常优异，在ILSVRC2015比赛中夺得头筹。

提出残差学习的思想。传统的卷积网络或者全连接网络在信息传递的时候或多或少会存在信息丢失，损耗等问题，同时还有导致梯度消失或者梯度爆炸，导致很深的网络无法训练。ResNet在一定程度上解决了这个问题，通过直接将输入信息绕道传到输出，保护信息的完整性，整个网络只需要学习输入、输出差别的那一部分，简化学习目标和难度。VGGNet和ResNet的对比如下图所示。ResNet最大的区别在于有很多的旁路将输入直接连接到后面的层，这种结构也被称为shortcut或者skip connections。

\input{Img/resnet}

\input{Img/backbone}

\section{SE-Inception}
