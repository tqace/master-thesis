% This is file `seuthesix.tex',
% This file is the source of the documentation of the `seuthesix' class.
% Copyright (c) 2016 James Fan, email: zhimengfan1990@163.com
% License: GNU General Public License, version 3
%This file is part of ``seuthesix'' package.
%``seuthesix'' is free software: you can redistribute it and/or modify
%it under the terms of the GNU General Public License as published by
%the Free Software Foundation, either version 3 of the License, or
%(at your option) any later version.
%``seuthesix'' is distributed in the hope that it will be useful,
%but WITHOUT ANY WARRANTY; without even the implied warranty of
%MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
%GNU General Public License for more details.
%
%You should have received a copy of the GNU General Public License
%along with this program.  If not, see <http://www.gnu.org/licenses/>.

\documentclass[figurelist,tablelist,algorithmlist,nomlist,masters]{Style/seuthesix}
\usepackage{hologo}
\usepackage{pdfpages}
\usepackage{gensymb}
\usepackage{enumitem}
\setlist[enumerate]{wide=\parindent}% only indent the first line
\setlist[itemize]{wide=\parindent}% only indent the first line
\setlist{nosep}
\setcounter{secnumdepth}{3}

\begin{document}
\input{Tex/Frontpage}
\begin{abstract}{服装检索，深度学习，注意力机制，多粒度}
  随着社会开始走向智能化，网购成为人们的主要消费方式，传统的基于文本的服装检索算法（TBIR）越来越不能满足人们精细的检索需求。得益于深度学习的蓬勃发展，基于内容的服装检索算法（CBIR）
  成为主流。近阶段，对图像局部区域信息的抓取和对齐是该领域研究的一个热门方向，因此，设计出一个有效提取服装细节信息的深度网络具有很大的实用价值。

  本文在总结已有工作的基础上，针对服装图像局部语义信息的提取和对齐问题，作出了以下贡献：
  \begin{itemize}
    \item[1.]提出基于注意力机制的局部对齐网络。该网络采用多个局部分支并行的架构，单个分支是一个基于注意力模块的局部特征提取器，注意力模块包含空间注意力和通道注意力
      两个子模块，可以互相促进更好的学习注意力分布，每个分支虽然结构相同，但是可以自适应的学习得到不同的注意力分布图。此外，基于在线三元组损失（Online Triplet Loss）
      ，提出一种跨域的样本挖掘算法，该算法基于在源域训练的模型，可以在目标域挖取并标注无标签样本，用于新一轮的训练，可以有效适应目标域的数据分布。
    \item[2.]提出基于多粒度切分的局部对齐网络。对原图切分得到的局部区域输入网络并用分类损失做监督会面临过拟合的问题，通过对特征图感受野的分析，本方法对特征图实施
      切分操作以提取局部信息，模型性能得到大幅提升。采用环形切分的创新性切分方式，并通过与横向、纵向切分方式的组合，保证局部关键信息特征的完整性。另外，启发式的
      提出多粒度的切分策略，通过融合不同的切分尺度的局部特征，有效提升模型的表达能力。
  \end{itemize}
\end{abstract}
\begin{englishabstract}{Clothing retrieval, Deep learning, Attention mechanism, Multi-granularity}
  With the development of society towards intelligent age, online shopping has become the main way of consumption, the traditional Text-Based Image Retrieval(TBIR)
  algorithm can no longer meet the increasingly comprehensive needs when people doing online shopping. Benefits from the flourishing of deep learning, 
  Content-Based Image Retrieval(CBIR) has become mainstream. Recently, grasping and aligning the local area information of the input  clothing image is a 
  popular direction in this field. 
  
  On the basis of summarizing the existing work, following contributions are made to the extraction and alignment of local information 
  of clothing images:
  \begin{itemize}
    \item[1.]Part alignment network based on attention mechanism is proposed. This network uses multiple local branch parallel architectures, 
      and one single branch is a local feature extractor based on the attention module. Although each branch has the same structure, it can be adaptively learned to obtain different attention maps.
      In addition, a cross-domain sample mining algorithm is proposed, which is based on the deep model trained in the source domain and can digging and labeling unlabeled samples in the target domain.
    \item[2.]Multi-granularity part alignment network is proposed. This method performs a partition operation on the feature map rather than the input image to extract local information, 
      and the performance of is improved by a large margin. And through the combination of horizontal, vertical and annular partition methods to ensure the integrity of the key part information.
      In addition, The proposed multi-granularity partition strategy effectively enhances the model by merging local features of different partition scales.
  \end{itemize}
\end{englishabstract}

\tableofcontents
\listofothers

\mainmatter

\input{Tex/Intro}
\input{Tex/Review}
\input{Tex/PartNet}
\input{Tex/MGN}
\input{Tex/Conclusion}

\bibliographystyle{Biblio/seuthesix} 
\bibliography{Biblio/ref}       
\end{document}
